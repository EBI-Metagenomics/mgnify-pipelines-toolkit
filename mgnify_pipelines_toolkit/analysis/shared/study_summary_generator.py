#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Copyright 2024 EMBL - European Bioinformatics Institute
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
from collections import defaultdict
import glob
import logging
from pathlib import Path

import pandas as pd

from mgnify_pipelines_toolkit.constants.db_labels import TAXDB_LABELS, ASV_TAXDB_LABELS

logging.basicConfig(level=logging.DEBUG)


def parse_args():

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i",
        "--indir",
        required=True,
        type=str,
        help="Input directory to where all the individual analyses subdirectories are if summarising, or where all the summaries are if merging",
    )
    parser.add_argument(
        "-r",
        "--runs",
        required=False,
        type=str,
        help="File containing successful analyses generated by the pipeline",
    )
    parser.add_argument(
        "-m",
        "--mode",
        required=True,
        type=str,
        choices=["summarise", "merge"],
        help="Mode to either summarise analyses (summarise), or merge multiple existing summaries (merge)",
    )
    parser.add_argument(
        "-o", "--output", required=True, type=str, help="Prefix to output"
    )

    args = parser.parse_args()

    INDIR = args.indir
    RUNS = args.runs
    MODE = args.mode
    OUTPUT = args.output

    return INDIR, RUNS, MODE, OUTPUT


def get_tax_file(run_acc, analyses_dir, db_label):

    tax_file = ""

    db_path = Path(f"{analyses_dir}/{run_acc}/taxonomy-summary/{db_label}")

    if db_path.exists():
        if db_label in TAXDB_LABELS:
            tax_file = Path(
                f"{analyses_dir}/{run_acc}/taxonomy-summary/{db_label}/{run_acc}_{db_label}.txt"
            )
            if not tax_file.exists():
                logging.error(
                    f"DB path exists but file doesn't - exiting. Path: {tax_file}"
                )
                exit(1)

            file_size = tax_file.stat().st_size
            if (
                file_size == 0
            ):  # Pipeline can generate files that are empty for ITS DBs (UNITE and ITSoneDB),
                # so need to skip those. Should probably fix that at some point
                tax_file = ""
        elif db_label in ASV_TAXDB_LABELS:
            # TODO will need to do ASV DB stuff later once we know how we want to do it. For now return empty file
            tax_file = ""

    return tax_file


def parse_one_tax_file(run_acc, tax_file):

    res_df = pd.DataFrame()

    with open(tax_file, "r") as fr:
        for line in fr:
            line = line.strip()
            temp_lst = line.split("\t")
            curr_count = temp_lst[0]
            curr_tax = ";".join(temp_lst[1:])

            res_df.loc[curr_tax, run_acc] = curr_count

    return res_df


def generate_db_summary(db_label, tax_files, output_prefix):

    df_list = []

    if db_label in TAXDB_LABELS:
        for run_acc, tax_file in tax_files.items():
            df_list.append(parse_one_tax_file(run_acc, tax_file))

        res_df = df_list[0]
        for df in df_list[1:]:
            res_df = res_df.join(df, how="outer")
        res_df = res_df.fillna(0)

    elif db_label in ASV_TAXDB_LABELS:
        # TODO will need to do ASV DB stuff later once we know how we want to do it. For now skip
        pass

    res_df.to_csv(
        f"{output_prefix}_{db_label}_study_summary.tsv",
        sep="\t",
        index_label="taxonomy",
    )


def summarise_analyses(runs_df, analyses_dir, output_prefix):

    all_db_labels = TAXDB_LABELS + ASV_TAXDB_LABELS
    for db_label in all_db_labels:

        tax_files = defaultdict(Path)
        for i in range(0, len(runs_df)):
            run_acc = runs_df.loc[i, "run"]
            tax_file = get_tax_file(run_acc, analyses_dir, db_label)

            if tax_file:
                tax_files[run_acc] = tax_file

        if (
            len(tax_files) > 1
        ):  # If at least two analyses have results from the current DB, generate a study-level summary for it
            generate_db_summary(db_label, tax_files, output_prefix)


def organise_study_summaries(all_study_summaries):

    summaries_dict = defaultdict(list)

    for summary in all_study_summaries:
        summary_path = Path(summary)
        summary_filename = summary_path.stem

        temp_lst = summary_filename.split("_")
        summary_db_label = temp_lst[1]

        summaries_dict[summary_db_label].append(summary_path)

    return summaries_dict


def merge_summaries(analyses_dir, output_prefix):

    # TODO: The way we grab all the summaries might change depending on how the prefect side does things
    all_study_summaries = glob.glob(f"{analyses_dir}/*_study_summary.tsv")

    summaries_dict = organise_study_summaries(all_study_summaries)

    for db_label, summaries in summaries_dict.items():
        if len(summaries) > 1:
            res_df = pd.read_csv(summaries[0], sep="\t", index_col=0)
            for summary in summaries[1:]:
                curr_df = pd.read_csv(summary, sep="\t", index_col=0)
                res_df = res_df.join(curr_df, how="outer")
                res_df = res_df.fillna(0)
                res_df = res_df.astype(int)

            res_df.to_csv(
                f"{output_prefix}_{db_label}_study_summary.tsv",
                sep="\t",
                index_label="taxonomy",
            )


def main():

    INDIR, RUNS, MODE, OUTPUT = parse_args()

    if MODE == "summarise":
        if not RUNS:
            logging.error(
                "Can't run in `summarise` mode without specifying -r/--runs - exiting."
            )
            exit(1)

        runs_df = pd.read_csv(RUNS, names=["run", "status"])
        summarise_analyses(runs_df, INDIR, OUTPUT)
    elif MODE == "merge":
        merge_summaries(INDIR, OUTPUT)
    else:
        logging.error("Mode can only be `summarise` or `merge` - exiting.")
        exit(1)


if __name__ == "__main__":
    main()
