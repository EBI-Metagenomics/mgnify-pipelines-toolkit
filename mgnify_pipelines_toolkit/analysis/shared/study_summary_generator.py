#!/usr/bin/env python
# -*- coding: utf-8 -*-

# Copyright 2024 EMBL - European Bioinformatics Institute
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
from collections import defaultdict
import glob
import logging
from pathlib import Path

import pandas as pd

from mgnify_pipelines_toolkit.constants.db_labels import TAXDB_LABELS, ASV_TAXDB_LABELS

logging.basicConfig(level=logging.DEBUG)


def parse_args():

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-i",
        "--indir",
        required=True,
        type=str,
        help="Input directory to where all the individual analyses subdirectories are if summarising, or where all the summaries are if merging",
    )
    parser.add_argument(
        "-r",
        "--runs",
        required=False,
        type=str,
        help="File containing successful analyses generated by the pipeline",
    )
    parser.add_argument(
        "-m",
        "--mode",
        required=True,
        type=str,
        choices=["summarise", "merge"],
        help="Mode to either summarise analyses (summarise), or merge multiple existing summaries (merge)",
    )
    parser.add_argument(
        "-p", "--prefix", required=True, type=str, help="Prefix to output"
    )

    args = parser.parse_args()

    INDIR = args.indir
    RUNS = args.runs
    MODE = args.mode
    PREFIX = args.prefix

    return INDIR, RUNS, MODE, PREFIX


def get_tax_file(run_acc, analyses_dir, db_label):

    tax_file = ""

    db_path = Path(f"{analyses_dir}/{run_acc}/taxonomy-summary/{db_label}")

    if db_path.exists():
        if db_label in TAXDB_LABELS:
            tax_file = Path(
                f"{analyses_dir}/{run_acc}/taxonomy-summary/{db_label}/{run_acc}_{db_label}.txt"
            )
            if not tax_file.exists():
                logging.error(
                    f"DB path exists but file doesn't - exiting. Path: {tax_file}"
                )
                exit(1)

            file_size = tax_file.stat().st_size
            if (
                file_size == 0
            ):  # Pipeline can generate files that are empty for ITS DBs (UNITE and ITSoneDB),
                # so need to skip those. Should probably fix that at some point
                tax_file = ""
        elif db_label in ASV_TAXDB_LABELS:
            # ASV tax files could have up to two files, one for each amplified region (maximum two from the pipeline).
            # So will need to handle this differently to closed-reference files
            asv_tax_files = glob.glob(
                f"{analyses_dir}/{run_acc}/taxonomy-summary/{db_label}/*.txt"
            )
            asv_tax_files = [
                Path(file) for file in asv_tax_files if "concat" not in file
            ]  # Have to filter out concatenated file if it exists

            tax_file = asv_tax_files

    # output will be a Path if closed-reference file, or a list if ASV file
    return tax_file


def parse_one_tax_file(run_acc, tax_file):

    res_df = pd.DataFrame()

    with open(tax_file, "r") as fr:
        for line in fr:
            line = line.strip()
            temp_lst = line.split("\t")
            curr_count = temp_lst[0]
            curr_tax = ";".join(temp_lst[1:])

            res_df.loc[curr_tax, run_acc] = curr_count

    return res_df


def generate_db_summary(db_label, tax_files, output_prefix):

    if db_label in TAXDB_LABELS:
        df_list = []

        for run_acc, tax_file in tax_files.items():
            df_list.append(parse_one_tax_file(run_acc, tax_file))

        res_df = df_list[0]
        for df in df_list[1:]:
            res_df = res_df.join(df, how="outer")
        res_df = res_df.fillna(0)

        res_df.to_csv(
            f"{output_prefix}_{db_label}_study_summary.tsv",
            sep="\t",
            index_label="taxonomy",
        )

    elif db_label in ASV_TAXDB_LABELS:

        amp_region_dict = defaultdict(list)

        for (
            run_acc,
            tax_file_asv_lst,
        ) in (
            tax_files.items()
        ):  # each `tax_file` will be a list containing at most two files (one for each amp_region)
            for tax_file in tax_file_asv_lst:
                amp_region = str(tax_file).split("_")[
                    -5
                ]  # there are a lot of underscores in these names... but it is consistent
                amp_region_df = parse_one_tax_file(run_acc, tax_file)
                amp_region_dict[amp_region].append(amp_region_df)

        for amp_region, amp_region_dfs in amp_region_dict.items():
            if (
                len(amp_region_dfs) > 1
            ):  # Need at least two analyses with this amp_region to bother with the summary
                amp_res_df = amp_region_dfs[0]
                for amp_df in amp_region_dfs[1:]:
                    amp_res_df = amp_res_df.join(amp_df, how="outer")
                amp_res_df = amp_res_df.fillna(0)

                amp_res_df.to_csv(
                    f"{output_prefix}_{db_label}_{amp_region}_asv_study_summary.tsv",
                    sep="\t",
                    index_label="taxonomy",
                )


def summarise_analyses(runs_df, analyses_dir, output_prefix):

    all_db_labels = TAXDB_LABELS + ASV_TAXDB_LABELS
    for db_label in all_db_labels:

        tax_files = defaultdict(Path)
        for i in range(0, len(runs_df)):
            run_acc = runs_df.loc[i, "run"]
            tax_file = get_tax_file(run_acc, analyses_dir, db_label)

            if tax_file:
                tax_files[run_acc] = tax_file

        if (
            len(tax_files) > 1
        ):  # If at least two analyses have results from the current DB, generate a study-level summary for it
            generate_db_summary(db_label, tax_files, output_prefix)


def organise_study_summaries(all_study_summaries):

    summaries_dict = defaultdict(list)

    for summary in all_study_summaries:
        summary_path = Path(summary)
        summary_filename = summary_path.stem

        temp_lst = summary_filename.split("_")
        if "asv_study_summary" in summary_filename:
            summary_db_label = "_".join(
                temp_lst[1:3]
            )  # For ASVs we need to include the amp_region in the label
        else:
            summary_db_label = temp_lst[
                1
            ]  # For closed reference, just the db_label is needed

        summaries_dict[summary_db_label].append(summary_path)

    return summaries_dict


def merge_summaries(analyses_dir, output_prefix):

    # TODO: The way we grab all the summaries might change depending on how the prefect side does things
    all_study_summaries = glob.glob(f"{analyses_dir}/*_study_summary.tsv")

    summaries_dict = organise_study_summaries(all_study_summaries)

    for db_label, summaries in summaries_dict.items():
        if len(summaries) > 1:
            res_df = pd.read_csv(summaries[0], sep="\t", index_col=0)
            for summary in summaries[1:]:
                curr_df = pd.read_csv(summary, sep="\t", index_col=0)
                res_df = res_df.join(curr_df, how="outer")
                res_df = res_df.fillna(0)
                res_df = res_df.astype(int)

            res_df = res_df.reindex(sorted(res_df.columns), axis=1)
            res_df.to_csv(
                f"{output_prefix}_{db_label}_study_summary.tsv",
                sep="\t",
                index_label="taxonomy",
            )


def main():

    INDIR, RUNS, MODE, PREFIX = parse_args()

    if MODE == "summarise":
        if not RUNS:
            logging.error(
                "Can't run in `summarise` mode without specifying -r/--runs - exiting."
            )
            exit(1)

        runs_df = pd.read_csv(RUNS, names=["run", "status"])
        summarise_analyses(runs_df, INDIR, PREFIX)
    elif MODE == "merge":
        merge_summaries(INDIR, PREFIX)
    else:
        logging.error("Mode can only be `summarise` or `merge` - exiting.")
        exit(1)


if __name__ == "__main__":
    main()
